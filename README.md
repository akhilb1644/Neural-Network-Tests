<h1>Observing Differences Between Different Optimizers</h1>

<p1>The following is Adam optimization</p1>

<img width="576" height="455" alt="image" src="https://github.com/user-attachments/assets/d1ff7659-e8bc-4ef9-9dd0-698f858ccc28" />

<p1>The following is Stochastic Gradient Descent</p1>

<img width="567" height="455" alt="image" src="https://github.com/user-attachments/assets/832aba8f-69e4-40d2-a0b1-2fbef554b817" />

<p1>The following is Adagrad</p1>

<img width="584" height="455" alt="image" src="https://github.com/user-attachments/assets/39cf6b5c-8d3e-487f-8445-2f8f5eea08de" />

<p1>As we can see, the best optimizer is Adam optimization</p1>
